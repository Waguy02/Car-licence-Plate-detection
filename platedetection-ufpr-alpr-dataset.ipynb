{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environnement","metadata":{}},{"cell_type":"code","source":"!pip install efficientnet_pytorch\n!pip install tensorflow\n# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:04:59.207444Z","iopub.execute_input":"2022-05-27T16:04:59.207800Z","iopub.status.idle":"2022-05-27T16:05:17.889022Z","shell.execute_reply.started":"2022-05-27T16:04:59.207712Z","shell.execute_reply":"2022-05-27T16:05:17.888067Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: efficientnet_pytorch in /opt/conda/lib/python3.7/site-packages (0.7.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.11.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.6.3)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\nRequirement already satisfied: typing-extensions<3.11,>=3.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.10.0.2)\nRequirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.19.5)\nRequirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.1.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.43.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.19.4)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (5.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.15.0)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.0.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.27.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.4.6)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (3.3.6)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.8.1)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (59.8.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.35.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.6.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.11.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.26.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.0.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2021.10.8)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.7.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (3.2.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nROOT_DIR=\"/kaggle/working\"\nDATASET_ROOT_DIR=\"/kaggle/input/ufpr-alpr/UFPR-ALPR dataset\"\nCLASS_PLATE=\"PLATE\"\nCLASSES =[CLASS_PLATE]+ [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]\nCLASSES_IDS = {c:i for i, c in enumerate(CLASSES)}\nNUM_CLASSES = len(CLASSES)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:05:17.891529Z","iopub.execute_input":"2022-05-27T16:05:17.893046Z","iopub.status.idle":"2022-05-27T16:05:17.900841Z","shell.execute_reply.started":"2022-05-27T16:05:17.893000Z","shell.execute_reply":"2022-05-27T16:05:17.899852Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from time import strftime\nimport sys\nimport logging\ndef setup_logger():\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    a_logger = logging.getLogger()\n    a_logger.setLevel(\"INFO\")\n    log_dir=os.path.join(ROOT_DIR,\"logs\",\"output_logs\")\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    output_file_handler = logging.FileHandler(os.path.join(log_dir,strftime(\"log_%d_%m_%Y_%H_%M.log\")))\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_handler.setFormatter(formatter)\n    a_logger.propagate=False\n    a_logger.addHandler(output_file_handler)\n    a_logger.addHandler(stdout_handler)\nsetup_logger()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:05:17.902379Z","iopub.execute_input":"2022-05-27T16:05:17.902638Z","iopub.status.idle":"2022-05-27T16:05:17.913119Z","shell.execute_reply.started":"2022-05-27T16:05:17.902604Z","shell.execute_reply":"2022-05-27T16:05:17.912385Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom matplotlib import pyplot as plt\n\ndef get_car_image(car_name):\n    return os.path.join(constants.DATA_DIR,f\"{car_name}.jpg\")\n\n\ndef car_annotations(car_name):\n    return os.path.join(constants.DATA_DIR, f\"{car_name}.xml\")\n\ndef collate_fn(batch):\n    \"\"\"\n    To handle the data loading as different images may have different number\n    of objects and to handle varying size tensors as well.\n    \"\"\"\n    return tuple(zip(*batch))\n\nBOX_COLOR = (255, 0, 0)  # Red\nTEXT_COLOR = (255, 255, 255)  # White\n\n\ndef visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n    \"\"\"Visualizes a single bounding box on the image\"\"\"\n    x_min, y_min, x_max,y_max=bbox\n\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n\n    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n    cv2.putText(\n        img,\n        text=class_name,\n        org=(x_min, y_min - int(0.3 * text_height)),\n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=0.35,\n        color=TEXT_COLOR,\n        lineType=cv2.LINE_AA,\n    )\n    return img\n\n\ndef visualize(image, bboxes, category_ids, category_id_to_name):\n    img = image.copy()\n    for bbox, category_id in zip(bboxes, category_ids):\n        class_name = category_id_to_name[category_id]\n        img = visualize_bbox(img, bbox, class_name)\n    plt.figure(figsize=(12, 12))\n    plt.axis('off')\n    plt.imshow(img)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:05:17.915300Z","iopub.execute_input":"2022-05-27T16:05:17.915800Z","iopub.status.idle":"2022-05-27T16:05:17.956829Z","shell.execute_reply.started":"2022-05-27T16:05:17.915763Z","shell.execute_reply":"2022-05-27T16:05:17.956159Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# the dataset class\nimport glob\nimport os\nfrom enum import Enum\nimport albumentations as A\nimport cv2\nimport cv2.cv2\nimport numpy as np\nfrom albumentations.pytorch import ToTensorV2\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import transforms\n\nINPUT_SIZE = (600,600)\nCROP_RATIO=1.0\n\n\nclass DatasetType(Enum):\n    TRAIN = \"training\"\n    VALID = \"validation\"\n    TEST = \"testing\"\nclass CarPlateDataset(Dataset):\n    def __init__(self, type: DatasetType = DatasetType.TRAIN):\n        self.type = type\n        self.dataset_dir = os.path.join(DATASET_ROOT_DIR, self.type.value)\n        self.tracks_dict = {}\n\n        self.tracks = []\n        self.transforms=A.Compose([\n            A.Resize(height=INPUT_SIZE[0], width=INPUT_SIZE[1], always_apply=True),\n#             A.CenterCrop(height=INPUT_SIZE[0],width=INPUT_SIZE[1],p=1.0, always_apply=True),\n            ToTensorV2(always_apply=True)\n        ],\n            bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n        )\n\n        self.read_all_data()\n\n    def __len__(self):\n        return len(self.tracks)\n\n    def read_all_data(self):\n        \"\"\"\n        Real all data from the dataset\n\n        :return:\n        \"\"\"\n        for file in glob.glob(f'{self.dataset_dir}/**/*.txt', recursive=True):\n            track = os.path.basename(file).split(\".\")[0]\n            annotation_file = file\n            image_file = file.replace(\".txt\", \".png\")\n            self.tracks_dict[track] = (image_file, annotation_file)\n            self.tracks.append(track)\n\n    def __getitem__(self, idx):\n        track = self.tracks[idx]\n        image_file, annotation_file = self.tracks_dict[track]\n        with open(annotation_file, \"r\") as f :\n            annotations = f.readlines()\n        plate_id = annotations[6].split(\":\")[1].strip().replace(\"-\", \"\")\n\n        labels = [CLASSES_IDS[CLASS_PLATE]] + [CLASSES_IDS[c] for c in plate_id]\n        image=cv2.imread(image_file)/255.0\n        # Parsing annotations\n        bboxes = []\n        line_plate = annotations[7].split(\":\")[1].strip()\n        plate_bbox = [float(x) for x in line_plate.split(\" \")]\n        plate_bbox = [plate_bbox[0], plate_bbox[1], plate_bbox[0]+plate_bbox[2], plate_bbox[1]+plate_bbox[3]]\n        bboxes.append(plate_bbox)\n\n        for line in annotations[8:]:\n            content = line.split(\":\")[1].strip()\n            bbox = [int(x) for x in content.split(\" \")]\n            bbox = [bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]]\n            bboxes.append(bbox)\n\n        target=self.transforms(\n            image=image,\n            labels=np.array(labels),\n            bboxes=np.array(bboxes)\n\n        )\n        target={\"image\":torch.as_tensor(target[\"image\"],dtype=torch.float32),\"labels\":torch.as_tensor(target[\"labels\"],dtype=torch.int64),\n                \"boxes\":torch.as_tensor(target[\"bboxes\"],dtype=torch.float32),\n                \"image_id\":torch.as_tensor(idx,dtype=torch.float32)}\n        return target[\"image\"], target\n","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:05:17.958126Z","iopub.execute_input":"2022-05-27T16:05:17.958355Z","iopub.status.idle":"2022-05-27T16:05:19.094935Z","shell.execute_reply.started":"2022-05-27T16:05:17.958324Z","shell.execute_reply":"2022-05-27T16:05:19.094184Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Network","metadata":{}},{"cell_type":"code","source":"import logging\nimport os\n\nimport torch\n\nimport torchvision\nfrom efficientnet_pytorch import EfficientNet\nfrom torch import nn\nfrom torchvision.models.detection.anchor_utils import AnchorGenerator\nfrom torchvision.models.detection.faster_rcnn import  FasterRCNN\n\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\nclass ALPRNetwork(nn.Module):\n    def __init__(self,model_name =\"FasterRCNN\",reset=False,load_best=False):\n        super().__init__()\n        self.model_name=model_name\n        self.reset=reset\n        self.load_best=load_best\n        self.setup_checkpoints()\n        self.setup_network()\n        self.setup_checkpoints()\n\n\n    #1. Setup Network archi\n    def setup_network(self):\n        # self.backbone = torchvision.models.__dict__[\"resnet18\"](pretrained=True)\n        # self.backbone= nn.Sequential(  *list(self.backbone.children())[:-1])  ## Remove the last layer of resnet as it is a classificatoin layer\n        \n        model_name = 'efficientnet-b7'\n        model=EfficientNet.from_pretrained(model_name)\n        conv_stem = torch.nn.Sequential(model._conv_stem)\n        bn = torch.nn.Sequential(model._bn0)\n        blocks = torch.nn.Sequential(*model._blocks)\n        conv_head = torch.nn.Sequential(model._conv_head)\n        #     conv_head.out_channels = 1280\n\n        #Freezing some layers\n        for p in conv_stem.parameters():p.requires_grad=False\n        for child in list(blocks.children())[:-1]:\n            for p in child.parameters():\n                p.requires_grad=False\n\n\n        self.backbone = torch.nn.Sequential(conv_stem, bn, blocks, conv_head)\n        self.backbone.out_channels = 2560\n\n        # Freezing some layers\n        anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n                                           aspect_ratios=((0.5, 1.0, 2.0),))\n        roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n        self.model = FasterRCNN(self.backbone,\n                                 num_classes=NUM_CLASSES,\n                                 rpn_anchor_generator=anchor_generator,\n                                 box_roi_pool=roi_pooler)\n\n    ##2. Model Saving/Loading\n    def load_state(self):\n        \"\"\"\n        Load model\n        :param self:\n        :return:\n        \"\"\"\n        if self.load_best and os.path.exists(self.save_best_file):\n            logging.info(f\"Loading best model state : {self.save_file}\")\n            self.load_state_dict(torch.load(self.save_file, map_location=device))\n            return\n\n        if os.path.exists(self.save_file):\n            logging.info(f\"Loading model state : {self.save_file}\")\n            self.load_state_dict(torch.load(self.save_file, map_location=device))\n    def save_state(self, best=False):\n        if best:\n            torch.save(self.state_dict(), self.save_best_file)\n        else:\n            torch.save(self.state_dict(), self.save_file)\n\n\n    ##3. Setupping directories for weights /logs ... etc\n    def setup_checkpoints(self):\n        \"\"\"\n        Checking and creating directories for weights storage\n        @return:\n        \"\"\"\n        self.save_path = os.path.join(ROOT_DIR, 'zoos')\n        self.model_dir = os.path.join(self.save_path, self.model_name)\n        self.save_file = os.path.join(self.model_dir, f\"{self.model_name}.pt\")\n        self.save_best_file = os.path.join(self.model_dir, f\"{self.model_name}_best.pt\")\n        if not os.path.exists(self.model_dir):\n            os.makedirs(self.model_dir)\n        elif not self.reset:\n            self.load_state()\n\n    def forward(self, *args, **kwargs):\n        return self.model(*args, **kwargs)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:05:19.096344Z","iopub.execute_input":"2022-05-27T16:05:19.096718Z","iopub.status.idle":"2022-05-27T16:05:19.152097Z","shell.execute_reply.started":"2022-05-27T16:05:19.096679Z","shell.execute_reply":"2022-05-27T16:05:19.151391Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Trainer ","metadata":{}},{"cell_type":"code","source":"import json\nimport logging\nimport os\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nimport tqdm\ndevice=\"cuda\"\nclass Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n\nclass Trainer:\n    def __init__(self, network, optimizer, nb_epochs,reset=False):\n        self.network = network\n        self.optimizer = optimizer\n        self.nb_epochs = nb_epochs\n\n        self.tb_dir=os.path.join(ROOT_DIR, 'logs','tensorboard',network.model_name)\n        self.info_file=os.path.join(self.tb_dir,'info.json')\n        self.reset=reset\n        if os.path.exists(self.info_file) and not self.reset:\n            self.infos=json.load(open(self.info_file))\n\n\n            ##Load learning rate:\n\n        else:\n            self.infos={'best_val_loss':float('inf'),'epoch':0,'train_itr':0,'lr':self.optimizer.param_groups[0]['lr'],\n\n                   }\n        self.summary_writer=SummaryWriter(self.tb_dir)\n\n\n\n    def train(self,train_dataloader,valid_dataloader):\n        self.network.to(device)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', factor=0.5, patience=5,verbose=True)\n        logging.info('Training started on device : {}. lr={}'.format(device, self.optimizer.param_groups[0]['lr']))\n        train_itr=self.infos['train_itr']\n\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = self.infos['lr']\n\n        start_epoch = self.infos['epoch'] if self.infos['epoch'] else 0\n        self.nb_epochs=start_epoch+self.nb_epochs\n        for epoch in range(start_epoch,self.nb_epochs):\n            self.network.train()\n            loss,loss_cf=self.train_epoch_loop(train_dataloader,epoch, train_itr)\n            eval_loss,eval_clf_loss=self.validate_loop(valid_dataloader,epoch)\n            scheduler.step(loss)\n            train_itr=train_itr+len(train_dataloader)\n            if eval_loss<self.infos['best_val_loss']:\n                self.infos['best_val_loss']=eval_loss\n            self.infos['epoch']=epoch\n            self.infos['train_itr']=train_itr\n            self.infos['lr']=self.optimizer.param_groups[0]['lr']\n            json.dump(self.infos,open(self.info_file,'w'))\n            logging.info('Epoch {}/{} : loss={:.4f} , loss_cf={:.4f} , eval_loss={:.4f} , eval_clf_loss={:.4f} , lr={:.6f}'.format(epoch,self.nb_epochs,loss,loss_cf,eval_loss,eval_clf_loss,self.infos['lr']))\n\n    def train_epoch_loop(self,dataloader, epoch, train_itr):\n        loss, loss_classification,loss_box_reg,loss_objectness=Averager(),Averager(),Averager(),Averager()\n        for batch in tqdm.tqdm(dataloader, desc='Training epoch {}/{}'.format(epoch, self.nb_epochs)):\n            train_itr += 1\n            self.optimizer.zero_grad()\n\n            images,targets=batch\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            loss_dict = self.network(images,targets)\n            losses = sum(loss for loss in loss_dict.values())\n            loss_value = losses.item()\n\n            loss.send(loss_value)\n            loss_classification.send(loss_dict['loss_classifier'].cpu().item())\n            loss_box_reg.send(loss_dict['loss_box_reg'].cpu().item())\n            loss_objectness.send(loss_dict['loss_objectness'].cpu().item())\n\n            self.summary_writer.add_scalar('Train/loss', loss_value, train_itr)\n            self.summary_writer.add_scalar('Train/loss_classification', loss_dict['loss_classifier'].cpu().item(),\n                                           train_itr)\n            self.summary_writer.add_scalar('Train/loss_box_reg', loss_dict['loss_box_reg'].cpu().item(), train_itr)\n            self.summary_writer.add_scalar('Train/loss_objectness', loss_dict['loss_objectness'].cpu().item(),train_itr)\n            losses.backward()\n            self.optimizer.step()\n        epoch_loss = loss.value\n        epoch_loss_classification = loss_classification.value\n        epoch_loss_box_reg = loss_box_reg.value\n        epoch_loss_objectness = loss_objectness.value\n        self.summary_writer.add_scalar('Train/epoch_loss', epoch_loss, epoch)\n        self.summary_writer.add_scalar('Train/epoch_loss_classification', epoch_loss_classification, epoch)\n        self.summary_writer.add_scalar('Train/epoch_loss_box_reg', epoch_loss_box_reg, epoch)\n        self.summary_writer.add_scalar('Train/epoch_loss_objectness', epoch_loss_objectness, epoch)\n        return epoch_loss, epoch_loss_classification\n\n\n    def validate_loop(self,valid_dataloader,epoch):\n\n        loss,loss_classification,loss_box_reg,loss_objectness=Averager(),Averager(),Averager(),Averager()\n        with torch.no_grad():\n            for batch in tqdm.tqdm(valid_dataloader,desc='Validating epoch {}'.format(epoch)):\n                images,targets=batch\n                images = list(image.to(device) for image in images)\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n                loss_dict=self.network(images,targets)\n                losses = sum(loss for loss in loss_dict.values())\n                loss_value = losses.item()\n                loss.send(loss_value)\n                loss_classification.send(loss_dict['loss_classifier'].cpu().item())\n                loss_box_reg.send(loss_dict['loss_box_reg'].cpu().item())\n                loss_objectness.send(loss_dict['loss_objectness'].cpu().item())\n\n                self.summary_writer.add_scalar('Valid/loss',loss_value,epoch)\n                self.summary_writer.add_scalar('Valid/loss_classification',loss_dict['loss_classifier'].cpu().item(),epoch)\n                self.summary_writer.add_scalar('Valid/loss_box_reg',loss_dict['loss_box_reg'].cpu().item(),epoch)\n                self.summary_writer.add_scalar('Valid/loss_objectness',loss_dict['loss_objectness'].cpu().item(),epoch)\n        epoch_loss=loss.value\n        epoch_loss_classification=loss_classification.value\n        epoch_loss_box_reg=loss_box_reg.value\n        epoch_loss_objectness=loss_objectness.value\n        self.summary_writer.add_scalar('Valid/epoch_loss',epoch_loss,epoch)\n        self.summary_writer.add_scalar('Valid/epoch_loss_classification',epoch_loss_classification,epoch)\n        self.summary_writer.add_scalar('Valid/epoch_loss_box_reg',epoch_loss_box_reg,epoch)\n        self.summary_writer.add_scalar('Valid/epoch_loss_objectness',epoch_loss_objectness,epoch)\n        return epoch_loss,epoch_loss_classification\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:05:19.153488Z","iopub.execute_input":"2022-05-27T16:05:19.153754Z","iopub.status.idle":"2022-05-27T16:05:19.272424Z","shell.execute_reply.started":"2022-05-27T16:05:19.153720Z","shell.execute_reply":"2022-05-27T16:05:19.271751Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Run Training","metadata":{}},{"cell_type":"code","source":"\n# %tensorflow_version 2.x\n%load_ext tensorboard\n%tensorboard --logdir logs/ ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:05:19.273730Z","iopub.execute_input":"2022-05-27T16:05:19.273991Z","iopub.status.idle":"2022-05-27T16:05:22.309653Z","shell.execute_reply.started":"2022-05-27T16:05:19.273956Z","shell.execute_reply":"2022-05-27T16:05:22.308756Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-22afb65fd7b1adcf\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-22afb65fd7b1adcf\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"code","source":"### from torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nreset=True\nnetwork=ALPRNetwork(reset=reset)\noptimizer=Adam(lr=5e-4, params=[param for param in network.parameters() if param.requires_grad])\ntrainer=Trainer(network, optimizer,nb_epochs=200,reset=reset)\ntrain_dataset,valid_dataset=CarPlateDataset(type=DatasetType.TRAIN),CarPlateDataset(type=DatasetType.VALID)\ntrain_dataloader,valid_dataloader=DataLoader(train_dataset,batch_size=1,shuffle=True,collate_fn=collate_fn,num_workers=2),\\\n                                  DataLoader(valid_dataset,batch_size=1,shuffle=True,collate_fn=collate_fn,num_workers=2)\ntrainer.train(train_dataloader,valid_dataloader)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T17:24:15.385375Z","iopub.execute_input":"2022-05-27T17:24:15.385702Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b7\n2022-05-27 17:24:19,280 - root - INFO - Training started on device : cuda. lr=0.0005\n","output_type":"stream"},{"name":"stderr","text":"Training epoch 0/200: 100%|██████████| 1800/1800 [14:15<00:00,  2.11it/s]\nValidating epoch 0: 100%|██████████| 900/900 [02:48<00:00,  5.36it/s]","output_type":"stream"},{"name":"stdout","text":"2022-05-27 17:41:22,355 - root - INFO - Epoch 0/200 : loss=0.5860 , loss_cf=0.2961 , eval_loss=0.1410 , eval_clf_loss=0.0925 , lr=0.000500\n","output_type":"stream"},{"name":"stderr","text":"\nTraining epoch 1/200: 100%|██████████| 1800/1800 [14:17<00:00,  2.10it/s]\nValidating epoch 1: 100%|██████████| 900/900 [02:46<00:00,  5.40it/s]","output_type":"stream"},{"name":"stdout","text":"2022-05-27 17:58:26,155 - root - INFO - Epoch 1/200 : loss=0.0882 , loss_cf=0.0591 , eval_loss=0.2136 , eval_clf_loss=0.1478 , lr=0.000500\n","output_type":"stream"},{"name":"stderr","text":"\nTraining epoch 2/200: 100%|██████████| 1800/1800 [14:06<00:00,  2.13it/s]\nValidating epoch 2:  41%|████      | 365/900 [01:08<01:38,  5.41it/s]IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nTraining epoch 3/200: 100%|██████████| 1800/1800 [14:06<00:00,  2.13it/s]\nValidating epoch 3: 100%|██████████| 900/900 [02:41<00:00,  5.57it/s]","output_type":"stream"},{"name":"stdout","text":"2022-05-27 18:32:08,186 - root - INFO - Epoch 3/200 : loss=0.0995 , loss_cf=0.0669 , eval_loss=0.2121 , eval_clf_loss=0.1355 , lr=0.000500\n","output_type":"stream"},{"name":"stderr","text":"\nTraining epoch 4/200:  78%|███████▊  | 1407/1800 [10:47<03:03,  2.14it/s]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}